{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2024BDA - Midterm Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View all datasets:\n",
      "['train_data.csv', 'bda2024_202203-202402_內容數據_新聞2-4.csv', 'bda2024_微股力_社群PKTD-2年-1.csv', 'test_data.csv', 'bda2024_202203-202402_內容數據_新聞3-3.csv', 'bda2024_微股力_社群PKTD-2年-6.csv', 'bda2024_202203-202402_討論數據_dcard.csv', 'bda2024_202203-202402_內容數據_新聞3-1.csv', 'bda2024_202203-202402_內容數據_新聞3-9.csv', '.DS_Store', 'bda2024_202203-202402_內容數據_新聞1-7.csv', 'bda2024_202203-202402_討論數據_mobile01-2.csv', 'bda2024_202203-202402_內容數據_新聞2-6.csv', 'bda2024_微股力_社群PKTD-2年-5.csv', 'bda2024_微股力_社群PKTD-2年-7.csv', 'bda2024_202203-202402_內容數據_新聞1-0.csv', 'bda2024_微股力_社群PKTD-2年-4.csv', 'bda2024_202203-202402_內容數據_新聞3-4.csv', 'bda2024_202203-202402_內容數據_新聞1-3.csv', 'bda2024_202203-202402_討論數據_mobile01-1.csv', 'bda2024_微股力_社群PKTD-2年-9.csv', 'bda2024_202203-202402_內容數據_新聞1-4.csv', 'bda2024_202203-202402_內容數據_新聞2-5.csv', 'bda2024_202203-202402_內容數據_新聞1-2.csv', 'bda2024_微股力_社群PKTD-2年-0.csv', 'bda2024_202203-202402_內容數據_新聞3-0.csv', 'bda2024_微股力_個股交易數據-2年.csv', 'bda2024_202203-202402_內容數據_新聞1-9.csv', 'bda2024_202203-202402_內容數據_新聞2-1.csv', 'bda2024_微股力_財報數據-2年.csv', 'bda2024_202203-202402_內容數據_新聞1-6.csv', 'bda2024_202203-202402_內容數據_新聞3-5.csv', 'bda2024_202203-202402_內容數據_新聞3-2.csv', 'bda2024_微股力_社群PKTD-2年-2.csv', 'bda2024_202203-202402_內容數據_新聞3-6.csv', 'bda2024_202203-202402_討論數據_ptt.csv', 'bda2024_202203-202402_內容數據_新聞3-7.csv', 'bda2024_202203-202402_內容數據_新聞2-2.csv', 'bda2024_202203-202402_內容數據_新聞1-5.csv', 'bda2024_202203-202402_內容數據_新聞2-7.csv', 'bda2024_微股力_籌碼數據-2年.csv', 'bda2024_微股力_社群PKTD-2年-3.csv', 'bda2024_202203-202402_內容數據_新聞1-8.csv', 'bda2024_202203-202402_內容數據_新聞1-1.csv', 'disc_df.csv', 'bda2024_202203-202402_內容數據_新聞2-0.csv', 'bda2024_202203-202402_內容數據_新聞2-9.csv', 'bda2024_202203-202402_內容數據_新聞2-3.csv', 'bda2024_202203-202402_內容數據_新聞3-8.csv', 'bda2024_202203-202402_內容數據_新聞2-8.csv', 'bda2024_微股力_社群PKTD-2年-8.csv']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset_dir = \"./dataset\"\n",
    "\n",
    "files = [f for f in os.listdir(dataset_dir) if os.path.isfile(os.path.join(dataset_dir, f))]\n",
    "print(\"View all datasets:\")\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(filepath, preview=True):\n",
    "    print(f\"\\n----- Loading {filepath}... -----\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"Size of dataframe: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    if preview:\n",
    "        print(df.head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因為要討論 ppt / 各種論壇的情緒指標和討論聲量和產業的關係，因此沒有用新聞內容的 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Loading ./dataset/bda2024_202203-202402_討論數據_dcard.csv... -----\n",
      "Size of dataframe: (231320, 10)\n",
      "Columns: ['id', 'forum', 's_name', 's_area_name', 'post_time', 'title', 'author', 'content', 'page_url', 'content_type']\n",
      "\n",
      "----- Loading ./dataset/bda2024_202203-202402_討論數據_mobile01-1.csv... -----\n",
      "Size of dataframe: (48725, 10)\n",
      "Columns: ['id', 'p_type', 's_name', 's_area_name', 'post_time', 'title', 'author', 'content', 'page_url', 'content_type']\n",
      "\n",
      "----- Loading ./dataset/bda2024_202203-202402_討論數據_mobile01-2.csv... -----\n",
      "Size of dataframe: (157939, 10)\n",
      "Columns: ['id', 'p_type', 's_name', 's_area_name', 'post_time', 'title', 'author', 'content', 'page_url', 'content_type']\n",
      "\n",
      "----- Loading ./dataset/bda2024_202203-202402_討論數據_ptt.csv... -----\n",
      "Size of dataframe: (50805, 9)\n",
      "Columns: ['id', 'p_type', 's_name', 's_area_name', 'post_time', 'title', 'author', 'content', 'page_url']\n",
      "\n",
      "----- Loading ./dataset/bda2024_微股力_個股交易數據-2年.csv... -----\n",
      "Size of dataframe: (1154225, 8)\n",
      "Columns: ['stock_name', 'stock_symbol', 'open', 'high', 'low', 'close', 'volume', 'date']\n",
      "  stock_name stock_symbol   open   high    low  close  volume  \\\n",
      "0         日馳         1526  47.55  48.45  47.55  48.30     138   \n",
      "1         日馳         1526  48.30  48.30  47.40  47.95     153   \n",
      "2         日馳         1526  48.45  48.70  47.80  48.10     120   \n",
      "3         日馳         1526  47.95  47.95  47.55  47.60     165   \n",
      "4         日馳         1526  47.65  47.65  45.30  45.65     514   \n",
      "\n",
      "                  date  \n",
      "0  2022-03-01 00:00:00  \n",
      "1  2022-03-02 00:00:00  \n",
      "2  2022-03-03 00:00:00  \n",
      "3  2022-03-04 00:00:00  \n",
      "4  2022-03-07 00:00:00  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18634/550884886.py:3: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n"
     ]
    }
   ],
   "source": [
    "disc_dcard_df = load_df(\"./dataset/bda2024_202203-202402_討論數據_dcard.csv\", preview=False)\n",
    "disc_dcard_df.rename(columns={'forum': 'p_type'}, inplace=True)    # Repair column name typo in data\n",
    "disc_m1_df = load_df(\"./dataset/bda2024_202203-202402_討論數據_mobile01-1.csv\", preview=False)\n",
    "disc_m2_df = load_df(\"./dataset/bda2024_202203-202402_討論數據_mobile01-2.csv\", preview=False)\n",
    "disc_ptt_df = load_df(\"./dataset/bda2024_202203-202402_討論數據_ptt.csv\", preview=False)\n",
    "disc_df = pd.concat([disc_dcard_df, disc_m1_df, disc_m2_df, disc_ptt_df], ignore_index=True)\n",
    "\n",
    "transaction_df = load_df(\"./dataset/bda2024_微股力_個股交易數據-2年.csv\")\n",
    "\n",
    "transaction_df['stock_symbol'] = transaction_df['stock_symbol'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_df = disc_df[[\"id\", \"post_time\", \"content\"]]\n",
    "\n",
    "disc_df[\"content\"] = disc_df[\"content\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Labeling\n",
    "\n",
    "AI 概念股：\n",
    "\n",
    "https://www.sinotrade.com.tw/richclub/hotstock/-65af4cb1880d9e29902a677f\n",
    "\n",
    "https://www.wantgoo.com/index/%5E435/stocks\n",
    "\n",
    "\n",
    "利用「全部 AI 概念股 n 天後的股價」減掉「全部 AI 概念股今天的股價」來當作判斷，若漲幅超過 m% 則判斷為漲。\n",
    "\n",
    "n = 7, m = 5 (暫定)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get target stocks\n",
    "\n",
    "target_stock_name_1 = ['廣達', '緯創', '台積電', '創意', '世芯-KY', '智原', '智邦', '信驊', '譜瑞-KY', '日月光投控', '台達電', '光寶科', '群光', '奇鋐', '金像電', '台燿', '嘉澤', '技嘉', '緯穎', '英業達', '鴻海', '聯發科', '聯茂', 'M31']\n",
    "target_stock_name_2 = ['AMAX-KY','緯創','凌群','創意','東元','英業達','原相','長佳智能','精誠','鴻海','華碩','微星','金寶','聯發科','世芯-KY','京元電子','美律','亞信','研華','台積電','宏碁','走著瞧-創','鈺創','廣達','凌華','零壹','台達電','樺漢','群電']\n",
    "\n",
    "#get rid of the duplicate stock names\n",
    "target_stock_name = target_stock_name_1 + target_stock_name_2\n",
    "target_stock_name = list(set(target_stock_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post_time</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1646109801927_F0DCU</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>定股美股ETF長期去抓報酬也是適合的投資工具方式</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1646109801940_F0DCU</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>最近剛申辦覺得定期定額投資美股很方便</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1646115341451_F0DCU</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>我三百買的 給你參考</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1646113689192_F0DCU</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>中鋼呢</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1646068286032_F0DCU</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>有100時候怎麼沒有選擇減碼落袋為安\\n現在用什麼心態在做當沖呢？？</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id   post_time                             content  label\n",
       "0  1646109801927_F0DCU  2022-03-01            定股美股ETF長期去抓報酬也是適合的投資工具方式    NaN\n",
       "1  1646109801940_F0DCU  2022-03-01                  最近剛申辦覺得定期定額投資美股很方便    NaN\n",
       "2  1646115341451_F0DCU  2022-03-01                          我三百買的 給你參考    NaN\n",
       "3  1646113689192_F0DCU  2022-03-01                                 中鋼呢    NaN\n",
       "4  1646068286032_F0DCU  2022-03-01  有100時候怎麼沒有選擇減碼落袋為安\\n現在用什麼心態在做當沖呢？？    NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_df['date'] = pd.to_datetime(transaction_df['date']).dt.date\n",
    "disc_df['post_time'] = pd.to_datetime(disc_df['post_time'])\n",
    "disc_df['post_time'] = disc_df['post_time'].dt.date\n",
    "\n",
    "#add a column \"label\" in transaction_df to indicate whether the stock price rises or not, default is null\n",
    "disc_df[\"label\"] = np.nan\n",
    "\n",
    "dates_list = sorted(list(set(transaction_df[\"date\"])))\n",
    "disc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the sum of the stock price in stock_name n days later is higher than the sum of stock price today by more than 5%, then label today's disc_df[\"rise\"] as 1\n",
    "# if the sum of the stock price in stock_name n days later is lower than the sum of stock price today by more than 5%, then label today's disc_df[\"rise\"] as -1\n",
    "# if the sum of the stock price in stock_name n days later is within 5% of the sum of stock price today, then label today's disc_df[\"rise\"] as 0\n",
    "\n",
    "n = 7\n",
    "m = 0.05\n",
    "\n",
    "for i in range(len(dates_list[:-7])):\n",
    "    #check the availability of stock price data of stock_name today\n",
    "    stock_data_today = transaction_df[transaction_df[\"date\"] == dates_list[i]]\n",
    "    stock_data_today_name = stock_data_today[\"stock_name\"].values\n",
    "\n",
    "    #check the availability of stock price data of stock_name n days later\n",
    "    stock_data_later = transaction_df[transaction_df[\"date\"] == dates_list[i + n]]\n",
    "    stock_data_later_name = stock_data_later[\"stock_name\"].values\n",
    "\n",
    "    #get the available stocks on both today and later\n",
    "    stock_name = [stock for stock in stock_data_today_name if stock in stock_data_later_name]\n",
    "\n",
    "    stock_name = [stock for stock in stock_name if stock in target_stock_name]\n",
    "    \n",
    "    if len(stock_name) == 0:\n",
    "        continue\n",
    "\n",
    "    #if stock_name is not available in stock data, then label today's disc_df[\"rise\"] as -100\n",
    "\n",
    "    #calculate the sum of stock price of stock_name today\n",
    "    stock_price_today = 0\n",
    "    for stock in stock_name:\n",
    "        if stock in stock_data_today_name:\n",
    "            stock_price_today += stock_data_today[stock_data_today[\"stock_name\"] == stock][\"open\"].values[0]\n",
    "\n",
    "    #calculate the sum of stock price of stock_name n days later\n",
    "    stock_price_later = 0\n",
    "    for stock in stock_name:\n",
    "        if stock in stock_data_later_name:\n",
    "            stock_price_later += stock_data_later[stock_data_later[\"stock_name\"] == stock][\"close\"].values[0]\n",
    "\n",
    "    #calculate the percentage change of stock price, and label the data in disc_df[\"rise\"]\n",
    "    percentage_change = (stock_price_later - stock_price_today) / stock_price_today\n",
    "    if percentage_change > m:\n",
    "        disc_df.loc[disc_df[\"post_time\"] == dates_list[i], \"label\"] = 1\n",
    "    elif percentage_change < -m:\n",
    "        disc_df.loc[disc_df[\"post_time\"] == dates_list[i], \"label\"] = -1\n",
    "    else:\n",
    "        disc_df.loc[disc_df[\"post_time\"] == dates_list[i], \"label\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of the data with disc_df[\"rise\"] == null\n",
    "disc_df = disc_df[disc_df[\"label\"].notnull()]\n",
    "\n",
    "# reassign the post_id\n",
    "disc_df[\"post_id\"] = range(len(disc_df))\n",
    "\n",
    "#export disc_df to csv\n",
    "disc_df.to_csv(\"dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document feature extraction\n",
    "找出具鑑別力 (扣除共通字詞) 的關鍵字列表，合起來建構向量空間\n",
    "\n",
    "1000維度（？），把所有資料轉換成以這個向量空間為主的向量（嗎）\n",
    "\n",
    "先取 5000 筆當作 training data 來建構具鑑別力 (扣除共通字詞) 的關 鍵字列表，合起來建構向量空間\n",
    "1000 筆當作 testing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "把文章內的空白、奇怪的字元去掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_text(document: str):\n",
    "    # remove html tags\n",
    "    CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    clean_document = re.sub(CLEANR, '', document)\n",
    "    clean_document = re.sub(\n",
    "        r'^https?:\\/\\/.*[\\r\\n]*', '', clean_document, flags=re.MULTILINE)  # remove urls\n",
    "    clean_document = re.sub(r\"\\s+\", \"\", clean_document,\n",
    "                            flags=re.UNICODE)  # remove white spaces\n",
    "    clean_document = clean_document.replace(\"\\n\", \"\") .replace(\"\\r\\n\", \"\")\n",
    "    # remove line terminator\n",
    "    clean_document = re.sub(r\"/[^\\x20-\\x7E]/gmi\", \"\", clean_document)\n",
    "\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    clean_document = re.sub(emoji_pattern, \"\", clean_document)\n",
    "    return clean_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post_time</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>post_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1646109801927_F0DCU</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>定股美股ETF長期去抓報酬也是適合的投資工具方式</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1646109801940_F0DCU</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>最近剛申辦覺得定期定額投資美股很方便</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1646115341451_F0DCU</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>我三百買的給你參考</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1646113689192_F0DCU</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>中鋼呢</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1646068286032_F0DCU</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>有100時候怎麼沒有選擇減碼落袋為安現在用什麼心態在做當沖呢？？</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id   post_time                           content  label  \\\n",
       "0  1646109801927_F0DCU  2022-03-01          定股美股ETF長期去抓報酬也是適合的投資工具方式    0.0   \n",
       "1  1646109801940_F0DCU  2022-03-01                最近剛申辦覺得定期定額投資美股很方便    0.0   \n",
       "2  1646115341451_F0DCU  2022-03-01                         我三百買的給你參考    0.0   \n",
       "3  1646113689192_F0DCU  2022-03-01                               中鋼呢    0.0   \n",
       "4  1646068286032_F0DCU  2022-03-01  有100時候怎麼沒有選擇減碼落袋為安現在用什麼心態在做當沖呢？？    0.0   \n",
       "\n",
       "   post_id  \n",
       "0        0  \n",
       "1        1  \n",
       "2        2  \n",
       "3        3  \n",
       "4        4  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"./dataset.csv\")\n",
    "dataset[\"content\"] = dataset[\"content\"].astype(str)\n",
    "dataset[\"content\"] = dataset[\"content\"].apply(lambda x : clean_text(x))\n",
    "dataset.to_csv(\"./clean_dataset.csv\", index=False)\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/369694 [00:00<?, ?it/s]/home/namwoam/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/namwoam/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['[', ']', 'a', '一點', '下', '下去', '不可', '不斷', '主義', '之間', '二話', '些', '亦', '亮話', '人意', '今', '令', '以來', '以外', '以後', '位', '何樂', '使', '例外', '保留', '倒', '假', '偶', '充', '兒', '全', '兩', '具體', '再', '凡', '分批', '刻', '刻間', '前', '加', '勿', '卻', '口兒', '古', '只', '叮', '呆呆', '呼', '唷', '問', '啪', '喔', '噠', '嚴', '外', '夠', '大張', '大面兒', '天', '天窗', '好', '少', '巧', '差', '已', '年', '年覆', '度', '式', '彈指', '得及', '忽', '恰', '情', '慢', '慣', '成', '或少', '手段', '打開', '抗拒', '抵', '挨', '挨家', '挨戶', '挨門', '接連', '換句', '擇', '敞開', '新', '旗鼓', '日', '日覆', '早', '時', '晚', '暗地', '會兒', '末', '樂乎', '樣', '樣子', '權', '次', '止', '正', '歸根', '每刻', '每時', '毫', '決', '況', '消', '為止', '烏', '無', '無到', '無阻', '熱', '爾', '特', '獨厚', '由此', '番', '皆', '益善', '盡力', '盡心', '目前', '相反', '相對', '真', '瞧', '知', '短', '社會', '種', '窮年', '竭力', '策略', '簡而言', '簡言', '累月', '結底', '綜', '總', '罷', '肩', '背', '背地', '至今', '舉', '苦', '藉', '裏', '言', '言道', '設', '話', '說', '說來', '請', '諸', '講', '越', '趕早', '趕晚', '身心', '近年', '述', '逐戶', '速', '進一步', '達', '量', '長期', '長此', '長話', '開', '開交', '間', '限', '難', '頭', '類', '風雨', '默默', '點'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████| 369694/369694 [1:07:04<00:00, 91.86it/s]\n"
     ]
    }
   ],
   "source": [
    "import monpa\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "monpa.use_gpu(True)\n",
    "dataset = pd.read_csv(\"./clean_dataset.csv\")\n",
    "dataset[\"content\"] = dataset[\"content\"].fillna(\"\")\n",
    "stopwords = [line.rstrip()\n",
    "             for line in open('./stopwords.txt', encoding='utf8')]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    use_idf=True, stop_words=stopwords, tokenizer=monpa.cut, max_features=feature_dim)\n",
    "X = vectorizer.fit_transform(tqdm(dataset[\"content\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m document_vector \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m      2\u001b[0m     X\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n\u001b[1;32m      3\u001b[0m document_vector[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m document_vector\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_vector.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "document_vector = pd.DataFrame(\n",
    "    X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "document_vector[\"label\"] = dataset[\"label\"]\n",
    "document_vector.to_csv(\"document_vector.csv\",index=False)\n",
    "document_vector.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       " 0.0    221157\n",
       " 1.0     83263\n",
       "-1.0     65274\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./document_vector.csv\")\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "#df = df.drop(df[df[\"label\"] == 0].index)\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(369694, 500) (369694,)\n"
     ]
    }
   ],
   "source": [
    "features = df.iloc[:, :-1].to_numpy()\n",
    "labels = df.iloc[:, -1].to_numpy()\n",
    "print(features.shape , labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features[:20000], labels[:20000], test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]..................*......*\n",
      "optimization finished, #iter = 24909\n",
      "obj = -78145.925479, rho = 1.000543\n",
      "nSV = 14358, nBSV = 7606\n",
      ".........*....*\n",
      "optimization finished, #iter = 13379\n",
      "obj = -25412.323271, rho = -0.999431\n",
      "nSV = 5648, nBSV = 2490\n",
      "............*.....*\n",
      "optimization finished, #iter = 17960\n",
      "obj = -27089.437300, rho = -0.999444\n",
      "nSV = 7948, nBSV = 2365\n",
      "Total nSV = 16517\n"
     ]
    }
   ],
   "source": [
    "cls = svm.SVC(kernel='poly', gamma=0.5, C=10,verbose=True , decision_function_shape='ovo').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cls.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.39      0.07      0.12       556\n",
      "         0.0       0.63      0.96      0.76      1249\n",
      "         1.0       0.70      0.04      0.07       195\n",
      "\n",
      "    accuracy                           0.62      2000\n",
      "   macro avg       0.57      0.36      0.32      2000\n",
      "weighted avg       0.57      0.62      0.52      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Classification with Deep Learning\n",
    "\n",
    "Use yiyanghkust/finbert-tone-chinese to classify text, the model is fine-tuned with financial domain knowledge, read: [link](https://arxiv.org/abs/1908.10063)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d8d3d96aad491cbc116f3264bfe63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1ee87af2c5419682ef228342ddf037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b806d73e0ee4391aab3e2e1c9bfd74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3875311c2d4d4936a5b6cd11e99b259e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e949b24f364ea9bb669f8a72820959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "import numpy as np\n",
    "model_path = \"yiyanghkust/finbert-tone-chinese\"\n",
    "new_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path, output_attentions=True)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "PipelineInterface = TextClassificationPipeline(\n",
    "    model=new_model, tokenizer=tokenizer, return_all_scores=True)\n",
    "label = PipelineInterface(\"改裝車燈廠巨鎧精密新廠啟用 宣示今年起進快速成長期\")\n",
    "\n",
    "\n",
    "def convert(nlp_result: list) -> int:\n",
    "    scores = np.array([cl[\"score\"] for cl in nlp_result[0]])\n",
    "    return (np.argmax(scores)+1) % 3-1\n",
    "\n",
    "print(convert(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>post_time</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>post_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1646109801927_F0DCU</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>定股美股ETF長期去抓報酬也是適合的投資工具方式</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1646109801940_F0DCU</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>最近剛申辦覺得定期定額投資美股很方便</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1646115341451_F0DCU</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>我三百買的給你參考</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1646113689192_F0DCU</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>中鋼呢</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1646068286032_F0DCU</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>有100時候怎麼沒有選擇減碼落袋為安現在用什麼心態在做當沖呢？？</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                   id   post_time  \\\n",
       "0           0  1646109801927_F0DCU  2022-03-01   \n",
       "1           1  1646109801940_F0DCU  2022-03-01   \n",
       "2           2  1646115341451_F0DCU  2022-03-01   \n",
       "3           3  1646113689192_F0DCU  2022-03-01   \n",
       "4           4  1646068286032_F0DCU  2022-03-01   \n",
       "\n",
       "                            content  label  post_id  \n",
       "0          定股美股ETF長期去抓報酬也是適合的投資工具方式    0.0        0  \n",
       "1                最近剛申辦覺得定期定額投資美股很方便    0.0        1  \n",
       "2                         我三百買的給你參考    0.0        2  \n",
       "3                               中鋼呢    0.0        3  \n",
       "4  有100時候怎麼沒有選擇減碼落袋為安現在用什麼心態在做當沖呢？？    0.0        4  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "doc_df = pd.read_csv(\"./clean_dataset.csv\")\n",
    "doc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = doc_df[\"content\"].to_list()\n",
    "labels = df.iloc[:, -1].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    contents[:20000], labels[:20000], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pred = [convert(PipelineInterface(str(X_test[i]), padding=True,\n",
    "                                      truncation=True)) for i in range(len(X_test))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.19      0.02      0.03       556\n",
      "         0.0       0.63      0.93      0.75      1249\n",
      "         1.0       0.12      0.06      0.08       195\n",
      "\n",
      "    accuracy                           0.59      2000\n",
      "   macro avg       0.31      0.33      0.29      2000\n",
      "weighted avg       0.46      0.59      0.48      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, nlp_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
